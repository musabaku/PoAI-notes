{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra \n",
    "\n",
    "Linear algebra is a branch of mathematics that is widely used throughout science and engineering. However, because linear algebra is a form of continuous rather than discrete mathematics, many computer scientists have little experience with it. A good understanding of linear algebra is essential for understanding and working with many machine learning algorithms, especially deep learning algorithms. We therefore precede our introduction to deep learning with a focused presentation of the key linear algebra prerequisites.\n",
    "\n",
    "## Scalars, Vectors, Matrices and Tensors\n",
    "The study of linear algebra involves several types of mathematical objects:\n",
    "\n",
    "### Scalars: \n",
    "A scalar is just a single number, in contrast to most of the other objects studied in linear algebra, which are usually arrays of multiple numbers.\n",
    "\n",
    "### Vectors: \n",
    "A vector is an array of numbers. The numbers are arranged in order. We can identify each individual number by its index in that ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/vector.png\" alt=\"Drawing\" style=\"width:200px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices: \n",
    "A matrix is a 2-D array of numbers, so each element is identified\n",
    "by two indices instead of just one. We usually give matrices upper-case\n",
    "variable names with bold typeface, such as A. If a real-valued matrix A has\n",
    "a height of m and a width of n, then we say that $A \\in R^{m×n} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/matrix.png\" alt=\"Drawing\" style=\"width:600px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors: \n",
    "   In some cases we will need an array with more than two axes.\n",
    "In the general case, an array of numbers arranged on a regular grid with a\n",
    "variable number of axes is known as a tensor. We denote a tensor named “A”\n",
    "with this typeface: **A**. We identify the element of A at coordinates (i, j, k)\n",
    "by writing $A_{i,j,k}$.\n",
    "\n",
    "### Transpose\n",
    "\n",
    "One important operation on matrices is the **transpose**. The transpose of a\n",
    "matrix is the mirror image of the matrix across a diagonal line, called the main\n",
    "diagonal, running down and to the right, starting from its upper left corner.  We denote the transpose of a\n",
    "matrix $A$ as $A^T$.\n",
    "\n",
    "- Vectors can be thought of as matrices that contain only one column. The transpose of a vector is therefore a matrix with only one row. Sometimes we define a vector by writing out its elements in the text inline as a row matrix, then using the transpose operator to turn it into a standard column vector, e.g., $x = [x_1, x_2, x_3 ]^T$.\n",
    "\n",
    "- A scalar can be thought of as a matrix with only a single entry. From this, we can see that a scalar is its own transpose: $a = a^T$.\n",
    "\n",
    "- We can add matrices to each other, as long as they have the same shape, just by adding their corresponding elements: $C = A + B$ where $C_{i,j} = A_{i,j} + B_{i,j}$.\n",
    "\n",
    "- We can also add a scalar to a matrix or multiply a matrix by a scalar, just by performing that operation on each element of a matrix: $D = a · B + c$ where $D_{i,j} = a · B_{i,j} + c$.\n",
    "\n",
    "In the context of deep learning, we also use some less conventional notation.\n",
    "We allow the addition of matrix and a vector, yielding another matrix: $C = A + b$,\n",
    "where $C_{i,j} = A_{i,j} + b_{j}$. In other words, the vector b is added to each row of the\n",
    "matrix. This shorthand eliminates the need to define a matrix with b copied into\n",
    "each row before doing the addition. This implicit copying of b to many locations\n",
    "is called **broadcasting**.\n",
    "\n",
    "## Multiplying Matrices and Vectors\n",
    "\n",
    "One of the most important operations involving matrices is multiplication of two\n",
    "matrices. The **matrix product** of matrices A and B is a third matrix C. In\n",
    "order for this product to be defined, A must have the same number of columns as\n",
    "B has rows. If A is of shape $m × n$ and B is of shape $n × p$, then C is of shape\n",
    "$m × p$. We can write the matrix product just by placing two or more matrices\n",
    "together, e.g.\n",
    "\n",
    "$$C = AB.$$\n",
    "\n",
    "The product operation is defined by\n",
    "\n",
    "$$C_{i,j} = \\sum_k A_{i,k}B_{k,j}.$$\n",
    "\n",
    "Note that the standard product of two matrices is not just a matrix containing\n",
    "the product of the individual elements. Such an operation exists and is called the\n",
    "**element-wise product** or **Hadamard product**, and is denoted as $A \\bigodot B$.\n",
    "\n",
    "The **dot product** between two vectors x and y of the same dimensionality\n",
    "is the matrix product $x^Ty$. We can think of the matrix product $C = AB$ as\n",
    "computing $C_{i,j}$ as the dot product between row i of A and column j of B.\n",
    "\n",
    "Matrix product operations have many useful properties that make mathematical\n",
    "analysis of matrices more convenient. For example, matrix multiplication is\n",
    "distributive:\n",
    "\n",
    "$$ A(B + C) = AB +AC.$$ \n",
    "\n",
    "It is also associative:\n",
    "\n",
    "$$ A(BC) = (AB)C.$$\n",
    "\n",
    "Matrix multiplication is not commutative (the condition AB = BA does not\n",
    "always hold), unlike scalar multiplication. However, the dot product between two\n",
    "vectors is commutative:\n",
    "\n",
    "$$x^Ty = y^Tx.$$\n",
    "\n",
    "The transpose of a matrix product has a simple form:\n",
    "\n",
    "$$(AB)^T = B^TA^T.$$\n",
    "\n",
    "This allows us to demonstrate the equation, by exploiting the fact that the value\n",
    "of such a product is a scalar and therefore equal to its own transpose:\n",
    "\n",
    "$$x^Ty =(x ^Ty)^T = y^Tx .$$\n",
    "\n",
    "We now know enough linear algebra notation to write down a system of linear\n",
    "equations:\n",
    "\n",
    "$$Ax = b$$ \n",
    "\n",
    "where $A \\in R^{m×n}$ is a known matrix, $b \\in R^m$ is a known vector, and $x \\in R^n$ is a\n",
    "vector of unknown variables we would like to solve for. Each element $x_i$ of x is one\n",
    "of these unknown variables. Each row of A and each element of b provide another\n",
    "constraint. We can rewrite the equation as:\n",
    "\n",
    "$$A_{1,:}x = b_1$$\n",
    "$$A_{2,:}x = b_2$$ \n",
    "$$. . .$$ \n",
    "$$A_{m,:}x = b_m$$ \n",
    "\n",
    "or, even more explicitly, as:\n",
    "\n",
    "$$A_{1,1}x_1 + A_{1,2}x_2 + · · · + A{1,n}x_n = b_1 $$\n",
    "$$A_{2,1}x_1 + A_{2,2}x_2 + · · · + A{2,n}x_n = b_2 $$\n",
    "$$. . .$$\n",
    "$$A_{m,1}x_1 + A_{m,2}x_2 + · · · + A{m,n}x_n = b_m $$\n",
    "\n",
    "### Identity and Inverse Matrices\n",
    "Linear algebra offers a powerful tool called matrix inversion that allows us to\n",
    "analytically solve the equation for many values of A.\n",
    "\n",
    "To describe matrix inversion, we first need to define the concept of an identity\n",
    "matrix. An **identity matrix** is a matrix that does not change any vector when we\n",
    "multiply that vector by that matrix. We denote the identity matrix that preserves\n",
    "n-dimensional vectors as $I_n$. Formally, $I_n \\in R^{n×n}$, and\n",
    "$∀x \\in R_n, I_nx = x$. \n",
    "\n",
    "The structure of the identity matrix is simple: all of the entries along the main\n",
    "diagonal are 1, while all of the other entries are zero. The matrix inverse of A is denoted as $A^{−1}$, and it is defined as the matrix such that $A^{−1}A = I_n$. \n",
    "\n",
    "We can now solve the equation by the following steps:\n",
    "\n",
    "$$Ax = b$$\n",
    "$$A^{−1}Ax = A^{−1}b$$\n",
    "$$I_nx = A^{−1}b$$\n",
    "$$x = A−1b.$$\n",
    "\n",
    "Of course, this process depends on it being possible to find $A^{−1}$. \n",
    "\n",
    "When $A^{−1}$ exists, several different algorithms exist for finding it in closed form.\n",
    "In theory, the same inverse matrix can then be used to solve the equation many\n",
    "times for different values of b . However, $A^{−1}$ is primarily useful as a theoretical\n",
    "tool, and should not actually be used in practice for most software applications.\n",
    "Because $A^{−1}$ can be represented with only limited precision on a digital computer,\n",
    "algorithms that make use of the value of b can usually obtain more accurate\n",
    "estimates of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Dependence and Span\n",
    "\n",
    "In order for $A^{−1}$ to exist, the equation must have exactly one solution for every\n",
    "value of b. However, it is also possible for the system of equations to have no\n",
    "solutions or infinitely many solutions for some values of b. It is not possible to\n",
    "have more than one but less than infinitely many solutions for a particular b; if\n",
    "both x and y are solutions then\n",
    "\n",
    "$$ z = αx + (1 − α)y $$\n",
    "\n",
    "is also a solution for any real α.\n",
    "\n",
    "To analyze how many solutions the equation has, we can think of the columns\n",
    "of A as specifying different directions we can travel from the **origin** (the point\n",
    "specified by the vector of all zeros), and determine how many ways there are of\n",
    "reaching b. In this view, each element of x specifies how far we should travel in\n",
    "each of these directions, with $x_i$ specifying how far to move in the direction of\n",
    "column i:\n",
    "\n",
    "$$ Ax =\\sum_i{x_iA_{:,i}}.$$\n",
    "\n",
    "In general, this kind of operation is called a **linear combination**. Formally, a\n",
    "linear combination of some set of vectors $\\{v^{(1)}, . . . , v^{(n)}\\}$ is given by multiplying\n",
    "each vector $v^{(i)}$ by a corresponding scalar coefficient and adding the results:\n",
    "\n",
    "$$\\sum_i{c_iv^{(i)}}.$$\n",
    "\n",
    "**The span** of a set of vectors is the set of all points obtainable by linear combination\n",
    "of the original vectors.\n",
    "\n",
    "Determining whether **Ax = b** has a solution thus amounts to testing whether b\n",
    "is in the span of the columns of A. This particular span is known as the **column\n",
    "space** or the range of A.\n",
    "\n",
    "Formally, this kind of redundancy is known as **linear dependence**. A set of\n",
    "vectors is **linearly independent** if no vector in the set is a linear combination\n",
    "of the other vectors. If we add a vector to a set that is a linear combination of\n",
    "the other vectors in the set, the new vector does not add any points to the set’s\n",
    "span. This means that for the column space of the matrix to encompass all of $R^m$,\n",
    "the matrix must contain at least one set of m linearly independent columns.\n",
    "\n",
    "Together, this means that the matrix must be **square**, that is, we require that\n",
    "m = n and that all of the columns must be linearly independent. A square matrix\n",
    "with linearly dependent columns is known as **singular**.\n",
    "\n",
    "So far we have discussed matrix inverses as being multiplied on the left. It is\n",
    "also possible to define an inverse that is multiplied on the right:\n",
    "\n",
    "$$AA^{−1} = I.$$ \n",
    "\n",
    "For square matrices, the left inverse and right inverse are equal.\n",
    "\n",
    "### Norms\n",
    "\n",
    "Sometimes we need to measure the size of a vector. In machine learning, we usually\n",
    "measure the size of vectors using a function called a **norm** . Formally, the $L^p$ norm\n",
    "is given by\n",
    "\n",
    "$$ ||x||_p = {(\\sum_i|x_i|^p)}^{\\frac{1}{p}} $$\n",
    "\n",
    "for $p \\in R,p \\geq 1$.\n",
    "\n",
    "Norms, including the $L^p$ norm, are functions mapping vectors to non-negative\n",
    "values. $O_n$ an intuitive level, the norm of a vector x measures the distance from\n",
    "the origin to the point x. More rigorously, a norm is any function f that satisfies\n",
    "the following properties:\n",
    "\n",
    "- f (x) = 0 ⇒ x = 0\n",
    "- f (x + y) ≤ f(x) + f (y) (the triangle inequality)\n",
    "- ∀α ∈ R, f (αx) = |α|f (x)\n",
    "\n",
    "The $L^2$ norm, with p = 2, is known as the Euclidean norm. It is simply the\n",
    "Euclidean distance from the origin to the point identified by x. The $L^2$ norm is\n",
    "used so frequently in machine learning that it is often denoted simply as ||x||, with\n",
    "the subscript 2 omitted. It is also common to measure the size of a vector using\n",
    "the squared $L^2$ norm, which can be calculated simply as $x^Tx$.\n",
    "\n",
    "The squared $L^2$ norm is more convenient to work with mathematically and\n",
    "computationally than the $L^2$ norm itself. For example, the derivatives of the\n",
    "squared $L^2$ norm with respect to each element of x each depend only on the\n",
    "corresponding element of x, while all of the derivatives of the $L^2$ norm depend\n",
    "on the entire vector. In many contexts, the squared $L^2$ norm may be undesirable\n",
    "because it increases very slowly near the origin. In several machine learning applications, it is important to discriminate between elements that are exactly\n",
    "zero and elements that are small but nonzero. In these cases, we turn to a function\n",
    "that grows at the same rate in all locations, but retains mathematical simplicity:\n",
    "the  $L^1$ norm. The $L^1$ norm may be simplified to\n",
    "\n",
    "$$||x||_1 = \\sum_i|x_i|.$$\n",
    "\n",
    "The $L^1$ norm is commonly used in machine learning when the difference between\n",
    "zero and nonzero elements is very important. Every time an element of x moves\n",
    "away from 0 by $\\epsilon$, the $L_1$ norm increases by $\\epsilon$.\n",
    "\n",
    "We sometimes measure the size of the vector by counting its number of nonzero\n",
    "elements. Some authors refer to this function as the “$L^0$ norm,” but this is incorrect\n",
    "terminology. The number of non-zero entries in a vector is not a norm, because\n",
    "scaling the vector by α does not change the number of nonzero entries. The $L^1$\n",
    "norm is often used as a substitute for the number of nonzero entries.\n",
    "\n",
    "One other norm that commonly arises in machine learning is the $L^{\\infty}$ norm,\n",
    "also known as the **max norm**. This norm simplifies to the absolute value of the\n",
    "element with the largest magnitude in the vector,\n",
    "\n",
    "$$||x||_{\\infty} = \\max_i |x_i |.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Kinds of Matrices and Vectors\n",
    "\n",
    "Some special kinds of matrices and vectors are particularly useful.\n",
    "\n",
    "**Diagonal matrices** consist mostly of zeros and have non-zero entries only along\n",
    "the main diagonal. Formally, a matrix D is diagonal if and only if $D_{i,j} = 0$ for\n",
    "all $i \\neq j$ . We have already seen one example of a diagonal matrix: the identity\n",
    "matrix, where all of the diagonal entries are 1. We write diag(v) to denote a square\n",
    "diagonal matrix whose diagonal entries are given by the entries of the vector v.\n",
    "Diagonal matrices are of interest in part because multiplying by a diagonal matrix\n",
    "is very computationally efficient. To compute diag(v)x, we only need to scale each\n",
    "element $x_i$ by $v_i$ . In other words, $diag(v)x = v \\bigodot x$. Inverting a square diagonal\n",
    "matrix is also efficient.  In many cases, we may derive some very general machine learning algorithm in terms of arbitrary matrices,\n",
    "but obtain a less expensive (and less descriptive) algorithm by restricting some\n",
    "matrices to be diagonal.\n",
    "\n",
    "A **symmetric matrix** is any matrix that is equal to its own transpose:\n",
    "\n",
    "$$A = A^T. $$\n",
    "\n",
    "Symmetric matrices often arise when the entries are generated by some function of\n",
    "two arguments that does not depend on the order of the arguments. For example,\n",
    "if A is a matrix of distance measurements, with Ai,j giving the distance from point\n",
    "i to point j, then $A_{i,j} = A_{j,i}$ because distance functions are symmetric.\n",
    "\n",
    "A **unit vector** is a vector with **unit norm**:\n",
    "\n",
    "$$ ||x||_2 = 1.$$\n",
    "\n",
    "A vector x and a vector y are **orthogonal** to each other if $x^Ty = 0$. If both\n",
    "vectors have nonzero norm, this means that they are at a 90 degree angle to each\n",
    "other. In $R^n$ , at most n vectors may be mutually orthogonal with nonzero norm.\n",
    "If the vectors are not only orthogonal but also have unit norm, we call them\n",
    "orthonormal.\n",
    "\n",
    "An orthogonal matrix is a square matrix whose rows are mutually orthonormal\n",
    "and whose columns are mutually orthonormal:\n",
    "\n",
    "$$A^{T}A = AA^T = I. $$\n",
    "\n",
    "This implies that\n",
    "\n",
    "$$A^{−1} = A^T,$$\n",
    "\n",
    "so orthogonal matrices are of interest because their inverse is very cheap to compute.\n",
    "Pay careful attention to the definition of orthogonal matrices. Counterintuitively,\n",
    "their rows are not merely orthogonal but fully orthonormal. There is no special\n",
    "term for a matrix whose rows or columns are orthogonal but not orthonormal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trace Operator\n",
    "The trace operator gives the sum of all of the diagonal entries of a matrix:\n",
    "\n",
    "$$ Tr(A) = \\sum_i A_{i,i} .$$\n",
    "\n",
    "The trace operator is useful for a variety of reasons. Some operations that are\n",
    "difficult to specify without resorting to summation notation can be specified using matrix products and the trace operator. For example, the trace operator provides\n",
    "an alternative way of writing the Frobenius norm of a matrix:\n",
    "\n",
    "$$ ||A||_F = \\sqrt{ Tr(AA^T)}. $$\n",
    "\n",
    "Writing an expression in terms of the trace operator opens up opportunities to\n",
    "manipulate the expression using many useful identities. For example, the trace\n",
    "operator is invariant to the transpose operator:\n",
    "\n",
    "$$Tr(A) = Tr(A^T).$$\n",
    "\n",
    "The trace of a square matrix composed of many factors is also invariant to\n",
    "moving the last factor into the first position, if the shapes of the corresponding\n",
    "matrices allow the resulting product to be defined:\n",
    "\n",
    "$$Tr(ABC) = Tr(CAB) = Tr(BCA)$$\n",
    "\n",
    "or more generally,\n",
    "\n",
    "$$ Tr(\\prod^n_{i=1}F(i)) = Tr(F^(n)\\prod^{n−1}_{i=1}F(i)). $$\n",
    "\n",
    "This invariance to cyclic permutation holds even if the resulting product has a\n",
    "different shape. For example, for $A \\in R^{m×n}$ and $B \\in R^{n×m}$, we have\n",
    "\n",
    "$$Tr(AB) = Tr(BA)$$\n",
    "\n",
    "even though $AB \\in R^{m×m}$ and $BA \\in R^{n×n}$.\n",
    "\n",
    "Another useful fact to keep in mind is that a scalar is its own trace: a = Tr(a).\n",
    "\n",
    "### The Determinant\n",
    "\n",
    "The determinant of a square matrix, denoted det(A), is a function mapping\n",
    "matrices to real scalars. The determinant is equal to the product of all the\n",
    "eigenvalues of the matrix. The absolute value of the determinant can be thought\n",
    "of as a measure of how much multiplication by the matrix expands or contracts\n",
    "space. If the determinant is 0, then space is contracted completely along at least\n",
    "one dimension, causing it to lose all of its volume. If the determinant is 1, then\n",
    "the transformation preserves volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalue and eigenvector\n",
    "\n",
    "Eigenvalue and eigenvector are probably one of the most important concepts in linear algebra. Who can expect a simple equation like $ Av = \\lambda v$ is so significant? From machine learning, quantum computing, and physic, many mathematical and engineering problems can be solved by finding the eigenvalue and eigenvectors of a matrix. Let’s not only discover what it is but also answer why it is so important. We will look into the Google PageRank to see how page ranking works.\n",
    "\n",
    "By definition, scalar λ and vector v are the eigenvalue and eigenvector of A if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin1.png\" alt=\"Drawing\" style=\"width:600px;\"/></th>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <th><img src=\"photos/lin2.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, Av lies along the same line as the eigenvector v.\n",
    "\n",
    "Here are some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin3.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <th><img src=\"photos/lin4.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Ax does not usually equal to λx. Only some exceptional vectors satisfy the condition. If the eigenvalue is greater than one, the corresponding Avᵢ will expand. If it is smaller than one, it will shrink.\n",
    "\n",
    "## Application\n",
    "\n",
    "But before getting into details, let’s pause and appreciate the beauty of such an abstract concept first. Many problems can be modeled with linear transformations with solutions derived from eigenvalues and eigenvectors. Let’s detail it with an abstract example first before real problems with a billion-dollar idea — Google’s PageRank. In many systems, we can express the properties in a vector with their rates of change linearly depend on the current properties (e.g. the population growth rate depends on the current population and GDP linearly.). The general equation is\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin5.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let’s take a guess on u(t) that satisfies the equation above. Since the derivative of an exponential function equals itself, we start with an exponential function of t and multiply it with a vector x — the output will be a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin6.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the calculation above, our solution for u(t) is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin7.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will find its complete solution. Our first order derivative equation is a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin8.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear functions, the complete solution is the linear combination of particular solutions. If u and v are the solutions, C₁u + C₂v is also the solution. From our previous example with eigenvalues λ = 4, -2 and -2, the complete solution will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin9.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a system will reach a stable state, then all eigenvalues have to be negative. At time t=0, we can measure the initial state u(0), say [u₀₁, u₀₂, u₀₃]ᵀ, and solve the constant C₁, C₂, and C₃."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin10.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an isolated example in demonstrating the power of eigenvalues. Nature seems to have an eigenvector cookbook when making its design. The famous time-independent Schrödinger equation is expressed with eigenvalues and eigenvectors. All observed properties are modeled by eigenvalues in quantum mechanics. They are many other examples including machine learning and one of the biggest eigenvector computed, Google PageRank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin11.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, many systems can be modeled as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin12.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s study the time sequence model a little more for the purpose of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin13.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we assume the initial state u₀ to be an eigenvector of A. Therefore, the future states can be computed as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin14.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, we can simplify the calculation by replacing the power of a matrix (Aᵏ) with the power of a scalar. Next, consider A has n linearly independent eigenvectors which form a basis of Rⁿ. We can decompose any vector of Rⁿ into this basis and simplify the calculation by computing the power of the eigenvalue again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin15.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a system will reach a stable state, we should expect λᵢ to be smaller or equal to 1. To compute the stable state, we can ignore terms with λᵢ smaller than 1 and just find the eigenvector associated with λᵢ = 1.\n",
    "\n",
    "Let’s discuss a real multi-billion idea to realize its full potential. Let’s simplify the discussion which assumes the whole internet contains only three web pages. The element Aᵢⱼ of a matrix A is the probability of a user going to page i when the user is on page j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin16.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sum up all the possibilities of the next page given a specific page, it equals 1. Therefore, all columns of A sum up to 1.0 and this kind of matrix is called the stochastic matrix, transition matrix or Markov matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin17.png\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov matrix has some important properties. The result of Ax or Aᵏx always sums up to one with its columns. This result indicates the chance of being on page 1, 2 and 3 respectively after each click. So it is obvious that it should sum up to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin17.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <th><img src=\"photos/lin18.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any Markov matrix A has an eigenvalue of 1 and other eigenvalues, positive or negative, will have their absolute values smaller than one. This behavior is very important. In our example,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin19.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Markov matrix, we can choose the eigenvector for λ=1 to have elements sum up to 1.0. Vectors v with elements sum up to one can be decomposed using the eigenvectors of A with c₁equals to 1 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin20.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since u₁, u₂, …, and $u_n$ are eigenvectors, Aᵏ can be replaced by λᵏ. Except for eigenvalue λ=1, the power of the eigenvalue (λᵏ) for a Markov matrix will diminish, as the absolute values of these eigenvalues are smaller than one. So the system reaches a steady state that approaches the eigenvector u₁ regardless of the initial state. And both Aᵏ and the steady state can be derived from the eigenvector u₁ as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin21.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the chance we land on page 1, 2 and 3 are about 0.41, 0.34 and 0.44 respectively. This concept has many potential applications. For instance, many problems can be modeled with Markov processes and a Markov/transition matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin22.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of eigenvalue & eigenvectors\n",
    "\n",
    "   - $A_x$ lies on the same line as the eigenvector x (same or opposite direction).\n",
    "   - The sum of eigenvalues equals the trace of a matrix (sum of diagonal elements).\n",
    "   - The product of eigenvalues equals the determinant.\n",
    "   - Both conditions above serve as a good insanity check on the calculations of eigenvalues.\n",
    "   - If no eigenvalue is repeated, all eigenvectors are linearly independent. Such an n × n matrix will have n eigenvalues and n linearly independent eigenvectors.\n",
    "   - If eigenvalues are repeated, we may or may not have all n linearly independent eigenvectors to diagonalize a square matrix.\n",
    "   - The number of positive eigenvalues equals the number of positive pivots.\n",
    "   - For Ax = λx,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin23.jpeg\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - If A is singular, it has an eigenvalue of 0. An invertible matrix has all eigenvalues non-zero.\n",
    "   - Eigenvalues and eigenvectors can be complex numbers.\n",
    "   - Projection matrices always have eigenvalues of 1 and 0 only. Reflection matrices have eigenvalues of 1 and -1.\n",
    "\n",
    "### More thoughts\n",
    "\n",
    "Eigenvalues quantify the importance of information along the line of eigenvectors. Equipped with this information, we know what part of the information can be ignored and how to compress information (SVD, Dimension reduction & PCA). It also helps us to extract features in developing machine learning models. Sometimes, it makes the model easier to train because of the reduction of tangled information. It also serves the purpose to visualize tangled raw data. Other applications include the recommendation systems or financial risk analysis. For example, we suggest movies based on your personal viewing behavior and others. We can also use eigenvectors to understand the correlations among data. Develop trends of the information and cluster information to find the common factors, like the combination of genes that triggers certain kind of disease. And all of them start from the simple equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"photos/lin1.png\" alt=\"Drawing\" style=\"width:800px;\"/></th>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
